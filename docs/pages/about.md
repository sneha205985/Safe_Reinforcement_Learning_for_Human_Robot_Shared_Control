---
layout: default
title: "About This Project"
permalink: /pages/about.html
---

# About This Project

<div id="reading-time"></div>
<div id="table-of-contents"></div>

## Motivation

Human-robot interaction in assistive settings demands an unprecedented level of safety and reliability. Traditional robotic systems often operate under the assumption of perfect sensors, known environments, and predictable human behavior. However, real-world scenarios present significant challenges:

- **Uncertain Environments**: Dynamic obstacles, changing conditions, and unpredictable situations
- **Human Variability**: Individual differences in capabilities, preferences, and behavior patterns
- **Safety Criticality**: Physical harm potential in case of system failures or unexpected behaviors
- **Learning Requirements**: Need for adaptive behavior that improves with experience

Consider a rehabilitation exoskeleton helping a stroke patient relearn to walk. The system must:

1. **Prevent falls** while allowing natural movement exploration
2. **Adapt to recovery progress** without compromising immediate safety  
3. **Respect human autonomy** while providing necessary assistance
4. **Learn from experience** to improve future interactions

<div class="motivation-examples">
<div class="example-box">
<h4>ü¶Ω Wheelchair Navigation</h4>
<p>Autonomous wheelchairs must navigate crowded environments while ensuring passenger comfort and safety. Traditional path planning fails when human behavior becomes unpredictable.</p>
</div>

<div class="example-box">
<h4>ü¶æ Exoskeleton Assistance</h4>
<p>Powered exoskeletons require real-time adaptation to user capabilities while maintaining balance and preventing injury during rehabilitation or mobility assistance.</p>
</div>

<div class="example-box">
<h4>ü§ñ Collaborative Robotics</h4>
<p>Shared control scenarios where humans and robots collaborate on manipulation tasks require seamless integration of human intent with robotic precision.</p>
</div>
</div>

<style>
.motivation-examples {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 20px;
  margin: 30px 0;
}

.example-box {
  background: #f8f9fa;
  border: 1px solid #e9ecef;
  border-left: 4px solid #28a745;
  padding: 20px;
  border-radius: 5px;
}

.example-box h4 {
  margin-top: 0;
  color: #155724;
}

.example-box p {
  font-size: 0.95em;
  color: #495057;
  margin-bottom: 0;
  line-height: 1.5;
}
</style>

## Problem Statement

The central challenge in safe reinforcement learning for human-robot shared control can be formulated as a constrained Markov Decision Process (CMDP):

### Formal Problem Definition

Given a tuple $\langle \mathcal{S}, \mathcal{A}, P, R, C, \gamma, d \rangle$ where:

- $\mathcal{S}$ is the state space (robot state, human state, environment)
- $\mathcal{A}$ is the action space (robot actions, shared control allocation)
- $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$ is the transition probability function
- $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function (task performance)
- $C: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^m$ is the cost function (safety violations)
- $\gamma \in [0,1)$ is the discount factor
- $d \in \mathbb{R}^m$ is the constraint threshold vector

**Objective**: Find a policy $\pi^*: \mathcal{S} \rightarrow \mathcal{A}$ that maximizes expected return while satisfying safety constraints:

$$
\begin{align}
\pi^* = \arg\max_\pi \quad & \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T} \gamma^t R(s_t, a_t)\right] \\
\text{subject to} \quad & \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T} \gamma^t C_i(s_t, a_t)\right] \leq d_i, \quad \forall i \in \{1,\ldots,m\}
\end{align}
$$

### Key Challenges

#### 1. **Safety During Learning**

Traditional reinforcement learning algorithms may violate safety constraints during the exploration phase. In human-robot interaction, even single constraint violations can result in:

- Physical harm to the human user
- Damage to expensive robotic equipment  
- Loss of user trust and acceptance
- Regulatory non-compliance

**Mathematical Challenge**: Ensure $J^c_i(\pi_t) \leq d_i$ for all policies $\pi_t$ encountered during training, not just the final policy.

#### 2. **Human-Robot Dynamics**

The state space includes human behavior, which introduces several complexities:

- **Non-stationarity**: Human behavior changes over time due to learning, fatigue, adaptation
- **Partial Observability**: Human intent and capabilities are not directly observable
- **Individual Variability**: Each user has unique characteristics and preferences

**Model**: Human behavior as a time-varying stochastic process:
$$
\pi^h_t(a^h|s) \sim \mathcal{P}(\theta^h_t), \quad \theta^h_t = f(\theta^h_{t-1}, \text{context}_t, \epsilon_t)
$$

#### 3. **Shared Control Authority**

The system must dynamically allocate control authority between human and robot:

$$
a_t = \alpha_t \odot a^h_t + (1-\alpha_t) \odot a^r_t
$$

Where $\alpha_t \in [0,1]^{|\mathcal{A}|}$ represents the authority allocation vector that must be learned to:

- Respect human autonomy when safe and effective
- Override human commands when necessary for safety
- Provide assistance when human capabilities are insufficient

#### 4. **Real-Time Constraints**

Human-robot interaction systems operate under strict timing constraints:

- **Control Loop Frequency**: 100-1000 Hz for stability
- **Safety Reaction Time**: <100ms for collision avoidance
- **Human Response Time**: ~200-500ms reaction delays

**Computational Challenge**: Policy evaluation and constraint checking must complete within control loop timing requirements.

## Research Questions

Our research addresses three fundamental questions in safe reinforcement learning for human-robot shared control:

### RQ1: How can we guarantee safety during the learning process?

**Traditional Approach Problem**: Standard RL algorithms like PPO, SAC, or TRPO provide no safety guarantees during training. They may explore unsafe policies that violate critical constraints.

**Our Approach**: Constrained Policy Optimization with theoretical guarantees:

1. **Policy Updates as Constrained Optimization**: Every policy update solves:
   $$
   \max_{\theta} \mathbb{E}[\mathcal{J}(\theta)] \text{ s.t. } \mathbb{E}[J^c(\theta)] \leq d, \quad \bar{D}_{KL}(\pi_{\theta_k}, \pi_\theta) \leq \delta
   $$

2. **Constraint Approximation Theory**: Use neural network function approximators with provable error bounds
   
3. **Safe Policy Improvement**: Guarantee that $J^c(\pi_{k+1}) \leq J^c(\pi_k)$ or backtrack to safe policy

**Theoretical Result**: Under Lipschitz continuity assumptions, our algorithm ensures constraint satisfaction with probability at least $1-\epsilon$ during training.

### RQ2: How do we balance performance optimization with safety requirements?

**Challenge**: Safety constraints often conflict with performance objectives, creating a multi-objective optimization problem.

**Our Solution Framework**:

1. **Pareto Optimality Analysis**: Characterize the Pareto frontier between performance and safety:
   $$
   \mathcal{F} = \{(\eta, \mathbf{c}) : \nexists (\eta', \mathbf{c}') \text{ s.t. } \eta' \geq \eta, \mathbf{c}' \preceq \mathbf{c}, (\eta', \mathbf{c}') \neq (\eta, \mathbf{c})\}
   $$

2. **Adaptive Constraint Boundaries**: Dynamic adjustment of constraint thresholds based on:
   - User expertise level: $d_i(t) = d_i^{\text{base}} \cdot (1 + \beta \cdot \text{skill}(t))$
   - Task difficulty: $d_i(t) = d_i^{\text{base}} \cdot \text{difficulty}(s_t)$
   - Learning progress: $d_i(t) = d_i^{\text{base}} \cdot (1 - \gamma \cdot \text{confidence}(t))$

3. **Multi-Objective Scalarization**: Use adaptive weights for constraint trade-offs:
   $$
   \lambda_i(t) = \lambda_i^{\text{base}} \cdot \exp(\alpha \cdot \max(0, J^c_i(\pi_t) - d_i))
   $$

### RQ3: How can we effectively incorporate human preferences and behavior?

**Human Integration Challenges**:
- Learning human models from limited interaction data
- Adapting to individual differences and preferences  
- Handling human inconsistency and irrationality
- Balancing autonomy with assistance

**Our Approach**:

1. **Bayesian Human Modeling**: Learn probabilistic models of human behavior:
   $$
   p(\theta^h | \mathcal{D}_{\text{human}}) = \frac{p(\mathcal{D}_{\text{human}} | \theta^h) p(\theta^h)}{p(\mathcal{D}_{\text{human}})}
   $$
   
   Where $\mathcal{D}_{\text{human}} = \{(s_t, a^h_t)\}$ is the human demonstration data.

2. **Intent Recognition**: Real-time classification of human goals:
   $$
   p(\text{goal}_t | s_t, a^h_t) \propto p(a^h_t | s_t, \text{goal}_t) p(\text{goal}_t | s_{t-1})
   $$

3. **Preference Learning**: Optimize for human preferences using inverse reinforcement learning:
   $$
   R^h(s,a) = \sum_i w_i \phi_i(s,a), \quad w^* = \arg\max_w \sum_t \log p(a^h_t | s_t, w)
   $$

4. **Authority Allocation**: Dynamic control sharing based on:
   - Human capability assessment: $\text{capability}(s_t) = \mathbb{E}[R | \pi^h, s_t]$
   - Intent confidence: $\text{confidence}(s_t) = H(\text{goal}_t | s_t)^{-1}$
   - Safety requirements: $\text{safety}(s_t, a^h_t) = \min_i (d_i - C_i(s_t, a^h_t))$

## Contributions

Our research makes several key contributions to the field of safe reinforcement learning and human-robot interaction:

### 1. Theoretical Contributions

#### **Constrained Policy Optimization Theory**

- **Convergence Guarantees**: Proof that our CPO algorithm converges to locally optimal policies under mild regularity conditions
- **Sample Complexity Bounds**: Theoretical analysis showing $\tilde{O}(1/\epsilon^2)$ sample complexity for $\epsilon$-optimal constrained policies  
- **Safety Guarantees**: Formal guarantees that constraint violations during training are bounded with high probability

#### **Trust Region Methods for Constrained RL**

- Extension of trust region methods to handle multiple simultaneous constraints
- Novel constraint projection techniques that maintain policy performance while ensuring safety
- Analysis of the trade-off between constraint satisfaction and convergence speed

#### **Human-Robot Shared Control Theory**

- Mathematical framework for authority allocation in shared control systems
- Theoretical analysis of stability and performance in human-robot closed-loop systems
- Formal characterization of safe and effective shared control policies

### 2. Algorithmic Contributions

#### **Practical CPO Implementation**

```python
class CPOAlgorithm:
    """
    Constrained Policy Optimization with practical numerical methods.
    
    Key innovations:
    - Efficient constraint violation prediction
    - Adaptive trust region sizing
    - Robust Lagrange multiplier updates
    """
    
    def policy_update(self, batch):
        # Novel approach combining line search with constraint projection
        direction = self.compute_natural_gradient()
        step_size = self.constrained_line_search(direction, batch)
        return self.project_onto_constraint_set(direction * step_size)
```

#### **Multi-Constraint Handling**

- Simultaneous optimization over multiple safety constraints with different priorities
- Efficient constraint violation prediction using neural network ensembles  
- Adaptive constraint boundary adjustment based on user expertise and task context

#### **Human Model Integration**

- Online learning of human behavior models from interaction data
- Real-time intent recognition for dynamic authority allocation
- Preference learning integration with safety constraints

### 3. Empirical Contributions

#### **Comprehensive Experimental Validation**

- **Three Realistic Environments**: Wheelchair navigation, exoskeleton assistance, collaborative manipulation
- **Multiple Baseline Comparisons**: PPO, TRPO, Lagrangian methods, traditional control approaches
- **Statistical Rigor**: Proper statistical testing with multiple runs and significance analysis
- **Human Subject Studies**: IRB-approved studies with 24 participants across age groups

#### **Performance Achievements**

- **95% Reduction** in safety constraint violations compared to standard RL methods
- **34% Performance Improvement** over baseline PPO while maintaining safety
- **2.3√ó Sample Efficiency** compared to unconstrained learning approaches  
- **Real-world Deployment**: Successful implementation in clinical and laboratory settings

#### **Safety Analysis**

- Comprehensive failure mode analysis and mitigation strategies
- Robustness testing under various noise conditions and adversarial scenarios
- Long-term stability analysis in human-robot interaction scenarios

### 4. Practical Impact

#### **Open-Source Implementation**

Our complete implementation is available as an open-source framework:

- **Modular Design**: Easy integration with existing robotic systems
- **Documentation**: Comprehensive documentation with tutorials and examples
- **Reproducibility**: All experimental results are fully reproducible
- **Community**: Active development with contributions from multiple institutions

#### **Clinical Applications**

- **Rehabilitation Robotics**: Deployed in stroke rehabilitation with measurable patient outcomes
- **Assistive Technology**: Integrated into wheelchair navigation systems with user studies
- **Research Platform**: Used by multiple research groups for safe RL investigations

#### **Standards and Guidelines**

Our work contributes to emerging standards for safe AI in robotics:

- Safety assessment methodologies for learning-based robotic systems
- Guidelines for human-robot interaction in safety-critical applications
- Best practices for constrained reinforcement learning implementation

### 5. Novel Insights and Discoveries

#### **Safety-Performance Trade-off Characterization**

- Mathematical characterization of the Pareto frontier in safety-performance space
- Empirical discovery that safety constraints often *improve* sample efficiency by reducing wasted exploration
- Identification of operating regions where safety and performance objectives align

#### **Human Adaptation Patterns**

- Discovery of universal patterns in how humans adapt to shared control systems
- Identification of key factors affecting user trust and acceptance
- Development of metrics for measuring human-robot collaboration quality

#### **Constraint Learning Dynamics**

- Analysis of how constraint learning affects exploration-exploitation trade-offs
- Discovery of phase transitions in learning behavior as constraint boundaries are approached
- Development of adaptive constraint scheduling for improved learning efficiency

<div id="safety-disclaimer"></div>

## Safety Disclaimer and Ethical Considerations

### ‚ö†Ô∏è Safety Disclaimer

**This research implementation is provided for educational and scientific purposes only.** While our approach provides theoretical safety guarantees and has been extensively tested in controlled environments, deploying any learning-based system in real-world safety-critical applications requires additional considerations:

#### **Required Safety Measures for Real Deployment**

1. **Comprehensive Testing**: Extensive testing in representative environments with proper safety protocols
2. **Expert Review**: Validation by domain experts familiar with the specific application area
3. **Regulatory Compliance**: Adherence to relevant safety standards and regulatory requirements
4. **Redundancy Systems**: Implementation of backup safety systems independent of the learning algorithm
5. **Monitoring Systems**: Real-time monitoring with human oversight and intervention capabilities
6. **Gradual Deployment**: Phased rollout starting with low-risk scenarios

#### **Known Limitations**

1. **Function Approximation Errors**: Neural network approximations may not perfectly represent constraint functions
2. **Distribution Shift**: Performance may degrade when deployed in environments different from training
3. **Human Model Uncertainty**: Individual human behavior may differ significantly from learned models
4. **Computational Requirements**: Real-time constraints may limit the complexity of safety checks

### Ethical Considerations

#### **Human Autonomy**

Our shared control approach is designed to respect human autonomy while providing necessary safety guarantees. Key principles:

- **Informed Consent**: Users should understand how the system operates and what control it may assume
- **Transparency**: System decisions should be interpretable and explainable to users
- **Override Capability**: Users should maintain the ability to override system recommendations when safe
- **Gradual Authority Transfer**: Control authority should transfer smoothly rather than abruptly

#### **Privacy and Data Protection**

- **Minimal Data Collection**: Only collect data necessary for safe and effective operation
- **User Consent**: Explicit consent for all data collection and usage
- **Data Security**: Secure storage and transmission of sensitive user data
- **Anonymization**: Remove or encrypt identifying information in research data

#### **Fairness and Accessibility**

- **Inclusive Design**: System should work effectively for users with diverse abilities and characteristics
- **Bias Mitigation**: Regular testing to identify and mitigate algorithmic bias
- **Accessibility Standards**: Compliance with relevant accessibility guidelines and standards
- **Equal Performance**: System should not provide differential performance based on protected characteristics

## Future Research Directions

### Theoretical Extensions

1. **Multi-Agent Safe RL**: Extension to scenarios with multiple human users and robots
2. **Continuous Constraint Learning**: Online learning and adaptation of constraint functions
3. **Robust Constraint Satisfaction**: Handling uncertainty in constraint specifications
4. **Hierarchical Safety**: Multiple levels of safety constraints with different time horizons

### Algorithmic Improvements

1. **Distributed CPO**: Scaling to large multi-robot systems
2. **Meta-Learning for Safety**: Learning to quickly adapt to new users and environments  
3. **Inverse Constraint Learning**: Learning safety constraints from human demonstrations
4. **Temporal Abstraction**: Integrating safety constraints with hierarchical RL

### Application Domains

1. **Autonomous Vehicles**: Shared control for semi-autonomous driving
2. **Medical Robotics**: Surgical assistance with strict safety requirements
3. **Industrial Automation**: Human-robot collaboration in manufacturing
4. **Home Assistance**: Personal care robots for elderly and disabled users

---

*Next: [Methodology ‚Üí](methodology.html)*