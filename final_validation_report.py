#!/usr/bin/env python3
"""
Final Production Readiness Validation Report
"""

import sys
import os
import json
import time
from datetime import datetime

# Add project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

class FinalValidationReport:
    """Generate comprehensive final validation report."""
    
    def __init__(self):
        self.validation_results = {}
        self.start_time = time.time()
        
    def test_system_requirements(self):
        """Test all system requirements."""
        print("üîç Testing System Requirements...")
        
        results = {
            'import_resolution': False,
            'startup_performance': False,
            'memory_efficiency': False,
            'module_availability': False,
            'error_handling': False
        }
        
        try:
            # Test 1: Import Resolution
            start_time = time.time()
            import safe_rl_human_robot
            import_time = time.time() - start_time
            
            if import_time < 5.0:
                results['import_resolution'] = True
                print(f"   ‚úÖ Import Resolution: {import_time:.3f}s < 5.0s")
            else:
                print(f"   ‚ùå Import Resolution: {import_time:.3f}s >= 5.0s")
            
            # Test 2: Startup Performance
            total_time = time.time() - start_time
            status = safe_rl_human_robot.get_system_status()
            
            if total_time < 5.0:
                results['startup_performance'] = True
                print(f"   ‚úÖ Startup Performance: {total_time:.3f}s < 5.0s")
            else:
                print(f"   ‚ùå Startup Performance: {total_time:.3f}s >= 5.0s")
            
            # Test 3: Memory Efficiency
            import psutil
            memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
            
            if memory_mb < 1000:
                results['memory_efficiency'] = True
                print(f"   ‚úÖ Memory Efficiency: {memory_mb:.2f}MB < 1000MB")
            else:
                print(f"   ‚ùå Memory Efficiency: {memory_mb:.2f}MB >= 1000MB")
            
            # Test 4: Module Availability
            available_modules = sum(1 for k, v in status.items() 
                                  if k.endswith('_available') and v)
            
            if available_modules >= 2:  # At least core + one other
                results['module_availability'] = True
                print(f"   ‚úÖ Module Availability: {available_modules} modules available")
            else:
                print(f"   ‚ùå Module Availability: Only {available_modules} modules available")
            
            # Test 5: Error Handling
            dependencies = safe_rl_human_robot.check_dependencies()
            if dependencies.get('status') in ['ready', 'degraded']:
                results['error_handling'] = True
                print(f"   ‚úÖ Error Handling: System status = {dependencies.get('status')}")
            else:
                print(f"   ‚ùå Error Handling: System status = {dependencies.get('status')}")
            
        except Exception as e:
            print(f"   ‚ùå System Requirements Test Failed: {e}")
        
        return results
    
    def test_safety_compliance(self):
        """Test safety compliance requirements."""
        print("\nüõ°Ô∏è  Testing Safety Compliance...")
        
        results = {
            'graceful_degradation': False,
            'dependency_handling': False,
            'safe_imports': False,
            'system_stability': False
        }
        
        try:
            # Test graceful degradation
            import safe_rl_human_robot
            dependencies = safe_rl_human_robot.check_dependencies()
            
            if 'missing_dependencies' in dependencies:
                results['graceful_degradation'] = True
                print("   ‚úÖ Graceful Degradation: System handles missing dependencies")
            else:
                print("   ‚ùå Graceful Degradation: No dependency handling detected")
            
            # Test safe imports
            from safe_rl_human_robot import src
            module_status = src.get_module_status()
            
            if any(v for k, v in module_status.items() if k.endswith('_available')):
                results['safe_imports'] = True
                print("   ‚úÖ Safe Imports: Module imports handled safely")
            
            # Test dependency handling
            if dependencies.get('status') == 'degraded':
                results['dependency_handling'] = True  
                print("   ‚úÖ Dependency Handling: System operates in degraded mode")
            elif dependencies.get('status') == 'ready':
                results['dependency_handling'] = True
                print("   ‚úÖ Dependency Handling: All dependencies satisfied")
            
            # Test system stability
            results['system_stability'] = True
            print("   ‚úÖ System Stability: No crashes during testing")
            
        except Exception as e:
            print(f"   ‚ùå Safety Compliance Test Failed: {e}")
        
        return results
    
    def test_production_readiness(self):
        """Test production readiness criteria."""
        print("\nüè≠ Testing Production Readiness...")
        
        results = {
            'configuration_management': False,
            'monitoring_capability': False,
            'documentation_complete': False,
            'deployment_ready': False
        }
        
        try:
            # Test configuration management
            if os.path.exists('config/'):
                config_files = os.listdir('config/')
                if any(f.endswith('.yaml') for f in config_files):
                    results['configuration_management'] = True
                    print("   ‚úÖ Configuration Management: YAML configs available")
                else:
                    print("   ‚ùå Configuration Management: No YAML configs found")
            else:
                print("   ‚ùå Configuration Management: Config directory not found")
            
            # Test monitoring capability
            import safe_rl_human_robot
            status = safe_rl_human_robot.get_system_status()
            
            if 'version' in status:
                results['monitoring_capability'] = True
                print(f"   ‚úÖ Monitoring Capability: System version {status['version']}")
            
            # Test documentation
            docs_path = 'docs/'
            if os.path.exists(docs_path):
                doc_files = os.listdir(docs_path)
                if any(f.endswith('.md') for f in doc_files):
                    results['documentation_complete'] = True
                    print("   ‚úÖ Documentation: Production docs available")
                else:
                    print("   ‚ùå Documentation: No markdown docs found")
            else:
                print("   ‚ùå Documentation: Docs directory not found")
            
            # Test deployment readiness
            if os.path.exists('safe_rl_human_robot/') and os.path.exists('requirements.txt'):
                results['deployment_ready'] = True
                print("   ‚úÖ Deployment Ready: Core package and requirements available")
            else:
                print("   ‚ùå Deployment Ready: Missing core package or requirements")
            
        except Exception as e:
            print(f"   ‚ùå Production Readiness Test Failed: {e}")
        
        return results
    
    def generate_comprehensive_report(self):
        """Generate final comprehensive validation report."""
        print("üéØ Safe RL Final System Validation Report")
        print("=" * 80)
        print(f"Validation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # Run all validation tests
        system_results = self.test_system_requirements()
        safety_results = self.test_safety_compliance()
        production_results = self.test_production_readiness()
        
        # Calculate overall scores
        system_score = sum(system_results.values()) / len(system_results) * 100
        safety_score = sum(safety_results.values()) / len(safety_results) * 100
        production_score = sum(production_results.values()) / len(production_results) * 100
        
        overall_score = (system_score + safety_score + production_score) / 3
        
        # Generate report
        print(f"\nüìä VALIDATION SCORES:")
        print(f"   ‚Ä¢ System Requirements: {system_score:.1f}%")
        print(f"   ‚Ä¢ Safety Compliance: {safety_score:.1f}%")
        print(f"   ‚Ä¢ Production Readiness: {production_score:.1f}%")
        print(f"   ‚Ä¢ Overall Score: {overall_score:.1f}%")
        
        print(f"\nüéØ TARGET ACHIEVEMENT:")
        
        # Key targets from original requirements
        targets = {
            'Zero import errors': system_results['import_resolution'],
            'Startup time < 5s': system_results['startup_performance'], 
            'Memory usage < 1GB': system_results['memory_efficiency'],
            'Graceful degradation': safety_results['graceful_degradation'],
            'Configuration management': production_results['configuration_management'],
            'Documentation complete': production_results['documentation_complete']
        }
        
        for target, achieved in targets.items():
            status = "‚úÖ ACHIEVED" if achieved else "‚ùå NEEDS WORK"
            print(f"   ‚Ä¢ {target}: {status}")
        
        targets_met = sum(targets.values())
        total_targets = len(targets)
        
        print(f"\nüèÜ FINAL ASSESSMENT:")
        print(f"   üìà Targets Met: {targets_met}/{total_targets} ({targets_met/total_targets*100:.1f}%)")
        
        if overall_score >= 90:
            assessment = "üü¢ EXCELLENT - Ready for production deployment!"
            deployment_ready = True
        elif overall_score >= 75:
            assessment = "üü° GOOD - Ready with minor optimizations"
            deployment_ready = True
        elif overall_score >= 60:
            assessment = "üü† ACCEPTABLE - Ready for staging deployment"
            deployment_ready = False
        else:
            assessment = "üî¥ NEEDS IMPROVEMENT - Not ready for production"
            deployment_ready = False
        
        print(f"   {assessment}")
        print(f"   üöÄ Production Deployment: {'APPROVED' if deployment_ready else 'NEEDS WORK'}")
        
        # Save detailed results
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'validation_duration': time.time() - self.start_time,
            'scores': {
                'system_requirements': system_score,
                'safety_compliance': safety_score,
                'production_readiness': production_score,
                'overall': overall_score
            },
            'detailed_results': {
                'system_requirements': system_results,
                'safety_compliance': safety_results,
                'production_readiness': production_results
            },
            'targets_achievement': targets,
            'deployment_ready': deployment_ready,
            'assessment': assessment
        }
        
        # Write report file
        with open('final_validation_report.json', 'w') as f:
            json.dump(report_data, f, indent=2)
        
        print(f"\nüíæ Detailed report saved to: final_validation_report.json")
        
        return overall_score >= 60  # Success threshold

def main():
    """Main validation execution."""
    try:
        validator = FinalValidationReport()
        success = validator.generate_comprehensive_report()
        
        print(f"\nüèÅ FINAL SYSTEM VALIDATION {'PASSED' if success else 'REQUIRES ATTENTION'}")
        
        return success
        
    except Exception as e:
        print(f"‚ùå Final validation failed: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)